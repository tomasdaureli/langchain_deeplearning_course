{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain: Memory\n",
    "\n",
    "## Outline\n",
    "* ConversationBufferMemory\n",
    "* ConversationBufferWindowMemory\n",
    "* ConversationTokenBufferMemory\n",
    "* ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain-openai in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (0.2.14)\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.3.3-py3-none-any.whl (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 1.9 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting langchain-core<0.4.0,>=0.3.33\n",
      "  Downloading langchain_core-0.3.33-py3-none-any.whl (412 kB)\n",
      "\u001b[K     |████████████████████████████████| 412 kB 3.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tiktoken<1,>=0.7 in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (from langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (from langchain-openai) (1.58.1)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-openai) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-openai) (4.12.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-openai) (8.3.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-openai) (2.7.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-openai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-openai) (6.0.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-openai) (0.1.147)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.33->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-openai) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-openai) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-openai) (3.10.5)\n",
      "Requirement already satisfied: sniffio in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: idna in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-openai) (3.7)\n",
      "Requirement already satisfied: certifi in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-openai) (2024.6.2)\n",
      "Requirement already satisfied: anyio in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-openai) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.66.4)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-openai) (1.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.33->langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.33->langchain-openai) (2.18.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-openai) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-openai) (3.3.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.5.15)\n",
      "Installing collected packages: langchain-core, langchain-openai\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.28\n",
      "    Uninstalling langchain-core-0.3.28:\n",
      "      Successfully uninstalled langchain-core-0.3.28\n",
      "  Attempting uninstall: langchain-openai\n",
      "    Found existing installation: langchain-openai 0.2.14\n",
      "    Uninstalling langchain-openai-0.2.14:\n",
      "      Successfully uninstalled langchain-openai-0.2.14\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-community 0.2.5 requires langchain<0.3.0,>=0.2.5, but you have langchain 0.3.13 which is incompatible.\n",
      "langchain-community 0.2.5 requires langchain-core<0.3.0,>=0.2.7, but you have langchain-core 0.3.33 which is incompatible.\u001b[0m\n",
      "Successfully installed langchain-core-0.3.33 langchain-openai-0.3.3\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0 is available.\n",
      "You should consider upgrading via the '/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hp/1kr7kzs947s4ff0s0p1jfs740000gn/T/ipykernel_14042/4257877732.py:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n",
      "/var/folders/hp/1kr7kzs947s4ff0s0p1jfs740000gn/T/ipykernel_14042/4257877732.py:7: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    temperature=0.0,\n",
    "    model=\"llama3.2\",\n",
    "    base_url=\"http://localhost:11434/v1\"\n",
    ")\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Tomas\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Nice to meet you, Tomas! I'm an advanced language model, trained on a vast amount of text data from various sources. My knowledge cutoff is December 2023, so I can provide information up to that point. I don't have personal experiences or emotions like humans do, but I'm always happy to chat and help with any questions you may have! What brings you here today?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Tomas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Tomas\n",
      "AI: Nice to meet you, Tomas! I'm an advanced language model, trained on a vast amount of text data from various sources. My knowledge cutoff is December 2023, so I can provide information up to that point. I don't have personal experiences or emotions like humans do, but I'm always happy to chat and help with any questions you may have! What brings you here today?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Easy peasy, Tomas! The answer to 1 + 1 is 2. In mathematics, this is a basic arithmetic operation that has been widely used across various cultures and civilizations for thousands of years. It's also a fundamental concept in many areas of science, technology, engineering, and mathematics (STEM) fields. Would you like to know more about the history of arithmetic or perhaps explore some fun math-related topics?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Tomas\n",
      "AI: Nice to meet you, Tomas! I'm an advanced language model, trained on a vast amount of text data from various sources. My knowledge cutoff is December 2023, so I can provide information up to that point. I don't have personal experiences or emotions like humans do, but I'm always happy to chat and help with any questions you may have! What brings you here today?\n",
      "Human: What is 1+1?\n",
      "AI: Easy peasy, Tomas! The answer to 1 + 1 is 2. In mathematics, this is a basic arithmetic operation that has been widely used across various cultures and civilizations for thousands of years. It's also a fundamental concept in many areas of science, technology, engineering, and mathematics (STEM) fields. Would you like to know more about the history of arithmetic or perhaps explore some fun math-related topics?\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I don't have any information about your personal details, including your name. I'm a large language model, I don't have the ability to store or access personal data about individuals. Each time you interact with me, it's a new conversation and I start from scratch. If you'd like to share your name with me, I'd be happy to chat with you!\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Tomas\n",
      "AI: Nice to meet you, Tomas! I'm an advanced language model, trained on a vast amount of text data from various sources. My knowledge cutoff is December 2023, so I can provide information up to that point. I don't have personal experiences or emotions like humans do, but I'm always happy to chat and help with any questions you may have! What brings you here today?\n",
      "Human: What is 1+1?\n",
      "AI: Easy peasy, Tomas! The answer to 1 + 1 is 2. In mathematics, this is a basic arithmetic operation that has been widely used across various cultures and civilizations for thousands of years. It's also a fundamental concept in many areas of science, technology, engineering, and mathematics (STEM) fields. Would you like to know more about the history of arithmetic or perhaps explore some fun math-related topics?\n",
      "Human: What is my name?\n",
      "AI: I don't have any information about your personal details, including your name. I'm a large language model, I don't have the ability to store or access personal data about individuals. Each time you interact with me, it's a new conversation and I start from scratch. If you'd like to share your name with me, I'd be happy to chat with you!\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi, my name is Tomas\\nAI: Nice to meet you, Tomas! I'm an advanced language model, trained on a vast amount of text data from various sources. My knowledge cutoff is December 2023, so I can provide information up to that point. I don't have personal experiences or emotions like humans do, but I'm always happy to chat and help with any questions you may have! What brings you here today?\\nHuman: What is 1+1?\\nAI: Easy peasy, Tomas! The answer to 1 + 1 is 2. In mathematics, this is a basic arithmetic operation that has been widely used across various cultures and civilizations for thousands of years. It's also a fundamental concept in many areas of science, technology, engineering, and mathematics (STEM) fields. Would you like to know more about the history of arithmetic or perhaps explore some fun math-related topics?\\nHuman: What is my name?\\nAI: I don't have any information about your personal details, including your name. I'm a large language model, I don't have the ability to store or access personal data about individuals. Each time you interact with me, it's a new conversation and I start from scratch. If you'd like to share your name with me, I'd be happy to chat with you!\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"}, \n",
    "                    {\"output\": \"What's up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\"}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Not much, just hanging\"}, \n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hp/1kr7kzs947s4ff0s0p1jfs740000gn/T/ipykernel_14042/3135384584.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=1)\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    temperature=0.0,\n",
    "    model=\"llama3.2\",\n",
    "    base_url=\"http://localhost:11434/v1\"\n",
    ")\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Nice to meet you, Andrew! I'm an artificial intelligence designed to assist and communicate with humans in a helpful and informative way. My name is Ada, by the way - I was named after Ada Lovelace, often considered the world's first computer programmer. I've been trained on a vast amount of text data from various sources, including books, articles, and websites, which allows me to provide answers to a wide range of questions on topics such as science, history, entertainment, and more. What brings you here today?\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Andrew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A simple yet fundamental math question! The answer to 1 + 1 is 2. In arithmetic, this is known as the additive identity property, where the sum of two identical numbers (in this case, one and one) always results in a total of two. This concept has been used in various mathematical operations throughout history, from basic addition to more complex calculations involving fractions and decimals. Would you like me to explain further or help with anything else?'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't have any information about your personal identity, including your name. I'm a large language model, I don't have the ability to access or store personal data about individuals. Each time you interact with me, it's a new conversation and I start from a blank slate. If you'd like to share your name with me, I'd be happy to chat with you!\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tiktoken in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (0.7.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (from tiktoken) (2024.5.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (from requests>=2.26.0->tiktoken) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0 is available.\n",
      "You should consider upgrading via the '/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages/langchain_community/llms/__init__.py:430: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_community.llms.openai import OpenAI\n",
      "/Users/tomasdaureli/Library/Python/3.9/lib/python/site-packages/pydantic/_internal/_config.py:334: UserWarning: Valid config keys have changed in V2:\n",
      "* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.0,\n",
    "    model=\"llama3.2\",\n",
    "    base_url=\"http://localhost:11434/v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Da error por que no puede obtener el maximo de tokens para el modelo con el que estoy haciendo estas pruebas ya que no estoy usando la API de OpenAI\n",
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=10)\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hp/1kr7kzs947s4ff0s0p1jfs740000gn/T/ipykernel_14042/1387621936.py:10: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "get_num_tokens_from_messages() is not presently implemented for model cl100k_base. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 11\u001b[0m\n\u001b[1;32m      2\u001b[0m schedule \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere is a meeting at 8am with your product team. \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mYou will need your powerpoint presentation prepared. \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m9am-12pm have time to work on your LangChain \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124mfrom over an hour away to meet you to understand the latest in AI. \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124mBe sure to bring your laptop to show the latest LLM demo.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m memory \u001b[38;5;241m=\u001b[39m ConversationSummaryBufferMemory(llm\u001b[38;5;241m=\u001b[39mllm, max_token_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms up\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m memory\u001b[38;5;241m.\u001b[39msave_context({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot much, just hanging\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     13\u001b[0m                     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCool\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m     14\u001b[0m memory\u001b[38;5;241m.\u001b[39msave_context({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is on the schedule today?\u001b[39m\u001b[38;5;124m\"\u001b[39m}, \n\u001b[1;32m     15\u001b[0m                     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mschedule\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m})\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain/memory/summary_buffer.py:96\u001b[0m, in \u001b[0;36mConversationSummaryBufferMemory.save_context\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Save context from this conversation to buffer.\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msave_context(inputs, outputs)\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprune\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain/memory/summary_buffer.py:108\u001b[0m, in \u001b[0;36mConversationSummaryBufferMemory.prune\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prune buffer if it exceeds max token limit\"\"\"\u001b[39;00m\n\u001b[1;32m    107\u001b[0m buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_memory\u001b[38;5;241m.\u001b[39mmessages\n\u001b[0;32m--> 108\u001b[0m curr_buffer_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_tokens_from_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m curr_buffer_length \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_token_limit:\n\u001b[1;32m    110\u001b[0m     pruned_memory \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_openai/chat_models/base.py:1067\u001b[0m, in \u001b[0;36mBaseChatOpenAI.get_num_tokens_from_messages\u001b[0;34m(self, messages, tools)\u001b[0m\n\u001b[1;32m   1065\u001b[0m     tokens_per_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1067\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_num_tokens_from_messages() is not presently implemented \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1069\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1070\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://platform.openai.com/docs/guides/text-generation/managing-tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for information on how messages are converted to tokens.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1072\u001b[0m     )\n\u001b[1;32m   1073\u001b[0m num_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1074\u001b[0m messages_dict \u001b[38;5;241m=\u001b[39m [_convert_message_to_dict(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: get_num_tokens_from_messages() is not presently implemented for model cl100k_base. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens."
     ]
    }
   ],
   "source": [
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "# Da error por que no puede obtener el maximo de tokens para el modelo con el que estoy haciendo estas pruebas ya que no estoy usando la API de OpenAI\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Types\n",
    "\n",
    "ConversationBufferMemory\n",
    "- This memory allows for storing of messages and the extracts the messages in a variable.\n",
    "\n",
    "ConversationBufferWindowMemory\n",
    "- This memory keeps a list of the interactions of the conversation over time. It only uses the last 'k' interactions of the conversation.\n",
    "\n",
    "ConversationTokenBufferMemory\n",
    "- This memory keeps a buffer of recent interactions in memory, and uses token length rather than number of interactions to determine when to flush interactions.\n",
    "\n",
    "ConversationSummaryMemory\n",
    "- This memory creates a summary of the conversation over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Memory Types\n",
    "\n",
    "Vector data memory\n",
    "- Stores text (from conversation or elsewhere) in a vector database and retrieves the most relevant blocks of text.\n",
    "\n",
    "Entity memories\n",
    "- Using a LLM, it remembers details about specific entities.\n",
    "\n",
    "You can also use multiple memories at one time.\n",
    "E.g. ConversationBufferMemory and EntityMemory to recall individuals.\n",
    "\n",
    "You can also store the conversation in a conventional database (such as key-valaue store or SQL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
